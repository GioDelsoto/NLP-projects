{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4f02df",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-21T11:13:20.345170Z",
     "iopub.status.busy": "2024-05-21T11:13:20.344657Z",
     "iopub.status.idle": "2024-05-21T11:13:20.351160Z",
     "shell.execute_reply": "2024-05-21T11:13:20.349761Z",
     "shell.execute_reply.started": "2024-05-21T11:13:20.345129Z"
    },
    "papermill": {
     "duration": 0.020832,
     "end_time": "2024-05-23T02:09:39.500310",
     "exception": false,
     "start_time": "2024-05-23T02:09:39.479478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " This notebook is my notes and codes of the youtube video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "## Papers: \n",
    "Attention is all you need: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "Deep Residual Learning: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
    "\n",
    "Dropout: https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a0cc66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:39.537337Z",
     "iopub.status.busy": "2024-05-23T02:09:39.536825Z",
     "iopub.status.idle": "2024-05-23T02:09:44.728521Z",
     "shell.execute_reply": "2024-05-23T02:09:44.727076Z"
    },
    "papermill": {
     "duration": 5.211943,
     "end_time": "2024-05-23T02:09:44.731400",
     "exception": false,
     "start_time": "2024-05-23T02:09:39.519457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F #To OneHotEncode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bcb72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:44.765556Z",
     "iopub.status.busy": "2024-05-23T02:09:44.764972Z",
     "iopub.status.idle": "2024-05-23T02:09:44.794393Z",
     "shell.execute_reply": "2024-05-23T02:09:44.793115Z"
    },
    "papermill": {
     "duration": 0.050573,
     "end_time": "2024-05-23T02:09:44.797581",
     "exception": false,
     "start_time": "2024-05-23T02:09:44.747008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = open('../input/shakespeare-tiny-input/input.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56acb4d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:44.832781Z",
     "iopub.status.busy": "2024-05-23T02:09:44.832319Z",
     "iopub.status.idle": "2024-05-23T02:09:44.838405Z",
     "shell.execute_reply": "2024-05-23T02:09:44.837141Z"
    },
    "papermill": {
     "duration": 0.026575,
     "end_time": "2024-05-23T02:09:44.841292",
     "exception": false,
     "start_time": "2024-05-23T02:09:44.814717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c91426f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:44.875805Z",
     "iopub.status.busy": "2024-05-23T02:09:44.875011Z",
     "iopub.status.idle": "2024-05-23T02:09:44.900903Z",
     "shell.execute_reply": "2024-05-23T02:09:44.899262Z"
    },
    "papermill": {
     "duration": 0.04645,
     "end_time": "2024-05-23T02:09:44.903462",
     "exception": false,
     "start_time": "2024-05-23T02:09:44.857012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b815100b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:44.938666Z",
     "iopub.status.busy": "2024-05-23T02:09:44.937505Z",
     "iopub.status.idle": "2024-05-23T02:09:44.946390Z",
     "shell.execute_reply": "2024-05-23T02:09:44.945176Z"
    },
    "papermill": {
     "duration": 0.029677,
     "end_time": "2024-05-23T02:09:44.949201",
     "exception": false,
     "start_time": "2024-05-23T02:09:44.919524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "#Create a mapping from characters to integers\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] #Encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227a000c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:44.983711Z",
     "iopub.status.busy": "2024-05-23T02:09:44.983296Z",
     "iopub.status.idle": "2024-05-23T02:09:45.386791Z",
     "shell.execute_reply": "2024-05-23T02:09:45.385466Z"
    },
    "papermill": {
     "duration": 0.423871,
     "end_time": "2024-05-23T02:09:45.389813",
     "exception": false,
     "start_time": "2024-05-23T02:09:44.965942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#lets encode the entire text dataset\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911713ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.430743Z",
     "iopub.status.busy": "2024-05-23T02:09:45.429828Z",
     "iopub.status.idle": "2024-05-23T02:09:45.437081Z",
     "shell.execute_reply": "2024-05-23T02:09:45.435752Z"
    },
    "papermill": {
     "duration": 0.034489,
     "end_time": "2024-05-23T02:09:45.440325",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.405836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6cdd8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.480894Z",
     "iopub.status.busy": "2024-05-23T02:09:45.480395Z",
     "iopub.status.idle": "2024-05-23T02:09:45.491416Z",
     "shell.execute_reply": "2024-05-23T02:09:45.490030Z"
    },
    "papermill": {
     "duration": 0.034305,
     "end_time": "2024-05-23T02:09:45.494088",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.459783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 #The block_size delimits the size of the chunk of text that we will use to train\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba679f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.531598Z",
     "iopub.status.busy": "2024-05-23T02:09:45.531210Z",
     "iopub.status.idle": "2024-05-23T02:09:45.539936Z",
     "shell.execute_reply": "2024-05-23T02:09:45.538701Z"
    },
    "papermill": {
     "duration": 0.031068,
     "end_time": "2024-05-23T02:09:45.543379",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.512311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "#We will use all sizes of input that is lower than block_size\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7437cd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.578643Z",
     "iopub.status.busy": "2024-05-23T02:09:45.578266Z",
     "iopub.status.idle": "2024-05-23T02:09:45.627583Z",
     "shell.execute_reply": "2024-05-23T02:09:45.626185Z"
    },
    "papermill": {
     "duration": 0.070074,
     "end_time": "2024-05-23T02:09:45.630531",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.560457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "outputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------\n",
      "When input is [24] the target: 43\n",
      "When input is [24, 43] the target: 58\n",
      "When input is [24, 43, 58] the target: 5\n",
      "When input is [24, 43, 58, 5] the target: 57\n",
      "When input is [24, 43, 58, 5, 57] the target: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "When input is [44] the target: 53\n",
      "When input is [44, 53] the target: 56\n",
      "When input is [44, 53, 56] the target: 1\n",
      "When input is [44, 53, 56, 1] the target: 58\n",
      "When input is [44, 53, 56, 1, 58] the target: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52] the target: 58\n",
      "When input is [52, 58] the target: 1\n",
      "When input is [52, 58, 1] the target: 58\n",
      "When input is [52, 58, 1, 58] the target: 46\n",
      "When input is [52, 58, 1, 58, 46] the target: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "When input is [25] the target: 17\n",
      "When input is [25, 17] the target: 27\n",
      "When input is [25, 17, 27] the target: 10\n",
      "When input is [25, 17, 27, 10] the target: 0\n",
      "When input is [25, 17, 27, 10, 0] the target: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many independent sequences will we process in parallel?\n",
    "block_size = 8 #what is the maximum context lenght for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is {context.tolist()} the target: {target}\")\n",
    "        \n",
    "        #We will not use all the possible context (like, 43 -> target 58). \n",
    "        #We increase the context but always starting at the begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126e6cf",
   "metadata": {
    "papermill": {
     "duration": 0.016117,
     "end_time": "2024-05-23T02:09:45.663540",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.647423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bigram BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef76b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.699195Z",
     "iopub.status.busy": "2024-05-23T02:09:45.698781Z",
     "iopub.status.idle": "2024-05-23T02:09:45.828733Z",
     "shell.execute_reply": "2024-05-23T02:09:45.827816Z"
    },
    "papermill": {
     "duration": 0.150751,
     "end_time": "2024-05-23T02:09:45.831207",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.680456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "        # nn.Embedding creates a 2d matrix with the logits values. It started with random number!!\n",
    "    \n",
    "    def forward(self, idx, targets = None):  # THIS CALLS __call__ method. So, m(xb, yb) = m.__call__(xb,yb)\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C) - Batch, time, channel\n",
    "        \n",
    "        # logits have a shape of [4,8,65] -> For each [4,8] characters, it returns the 65 logits values.\n",
    "        # We need to change the dimension for the cross_entropy call\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            \n",
    "        else:      \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)   \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B,C) ---> the -1 is to use only the last character - its a bigram\n",
    "            #apply softmas to get probabilites\n",
    "            probs = F.softmax(logits, dim = -1) #(B,C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples =1) #(B,1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=100)[0].tolist()))\n",
    "#With an uniform distribution, the loss would be -ln(1/65) = 4.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287195d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:45.873175Z",
     "iopub.status.busy": "2024-05-23T02:09:45.872402Z",
     "iopub.status.idle": "2024-05-23T02:09:48.920876Z",
     "shell.execute_reply": "2024-05-23T02:09:48.919548Z"
    },
    "papermill": {
     "duration": 3.07498,
     "end_time": "2024-05-23T02:09:48.923781",
     "exception": false,
     "start_time": "2024-05-23T02:09:45.848801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a pytorch optmizer\n",
    "optimizer = torch.optim.AdamW(m.parameters() , lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed0eada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:09:48.958834Z",
     "iopub.status.busy": "2024-05-23T02:09:48.958252Z",
     "iopub.status.idle": "2024-05-23T02:10:10.237241Z",
     "shell.execute_reply": "2024-05-23T02:10:10.236081Z"
    },
    "papermill": {
     "duration": 21.299618,
     "end_time": "2024-05-23T02:10:10.240015",
     "exception": false,
     "start_time": "2024-05-23T02:09:48.940397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c12303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.275631Z",
     "iopub.status.busy": "2024-05-23T02:10:10.275223Z",
     "iopub.status.idle": "2024-05-23T02:10:10.324733Z",
     "shell.execute_reply": "2024-05-23T02:10:10.323474Z"
    },
    "papermill": {
     "duration": 0.070792,
     "end_time": "2024-05-23T02:10:10.328064",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.257272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcde9b",
   "metadata": {
    "papermill": {
     "duration": 0.016463,
     "end_time": "2024-05-23T02:10:10.361916",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.345453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notes\n",
    "\n",
    "- We can run on CUDA or GPU -> CUDA is better\n",
    "\n",
    "    - hyperparameter device = 'cuda' if torch.cuda.is_avaiable() else 'cpu'\n",
    "    - If we use CUDA, we need to assure to move the data x,y = x.to(device), y.to(device). Also, when we create the model, we want to move to device m = model.to(device)\n",
    "    - Also, we need to create the variable on the device to generate : context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "    \n",
    "- We can set the model to evaluation \"model.eval()\" or to training \"model.train()\"\n",
    "\n",
    "\n",
    "- If we calculate the loss for each batch, it will be a little noise. It's better to use a estimative error, averaging the loss with respect to the batches. He used a function called \"estimate_loss()\". He used a @torch.no_grad() at the beggining of the function to tell pytorch to not compute the backprop there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3eeed",
   "metadata": {
    "papermill": {
     "duration": 0.016276,
     "end_time": "2024-05-23T02:10:10.394948",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.378672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f181eaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.430799Z",
     "iopub.status.busy": "2024-05-23T02:10:10.429619Z",
     "iopub.status.idle": "2024-05-23T02:10:10.437631Z",
     "shell.execute_reply": "2024-05-23T02:10:10.436793Z"
    },
    "papermill": {
     "duration": 0.02821,
     "end_time": "2024-05-23T02:10:10.439713",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.411503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fceb3",
   "metadata": {
    "papermill": {
     "duration": 0.016697,
     "end_time": "2024-05-23T02:10:10.472961",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.456264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Notes\n",
    "\n",
    "Time = number of characters used to generate the next one -> Each time that we increase the context, the \"time\" of x, increases\n",
    "Channels = \"information\" for each element of (batch, time) (i.e., logits, one-hot-encode)\n",
    "Batch = batch size\n",
    "\n",
    "In this tensor form, all the elements of T don't \"talk\" to each other. We would like to couple them to mix the information\n",
    "\n",
    "### Rules\n",
    "- The token in the fifth position should not talk to the tokens at 6, 7, 8 positions, ONLY with 1, 2, 3 and 4.\n",
    "- We want to mix the information on these channels (like taking the average of all previous steps)\n",
    "\n",
    "- Using the mean is a weak way to mix the infortation, because we lose a lot of information about the spacial distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a21e8089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.508683Z",
     "iopub.status.busy": "2024-05-23T02:10:10.508260Z",
     "iopub.status.idle": "2024-05-23T02:10:10.518374Z",
     "shell.execute_reply": "2024-05-23T02:10:10.517558Z"
    },
    "papermill": {
     "duration": 0.030611,
     "end_time": "2024-05-23T02:10:10.520608",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.489997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# But lets start taking the average\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev,0) #taking the average of all previous characters (taking a bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a21de65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.618158Z",
     "iopub.status.busy": "2024-05-23T02:10:10.617730Z",
     "iopub.status.idle": "2024-05-23T02:10:10.625754Z",
     "shell.execute_reply": "2024-05-23T02:10:10.624923Z"
    },
    "papermill": {
     "duration": 0.090472,
     "end_time": "2024-05-23T02:10:10.628054",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.537582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b8716a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.664137Z",
     "iopub.status.busy": "2024-05-23T02:10:10.663390Z",
     "iopub.status.idle": "2024-05-23T02:10:10.671557Z",
     "shell.execute_reply": "2024-05-23T02:10:10.670411Z"
    },
    "papermill": {
     "duration": 0.028818,
     "end_time": "2024-05-23T02:10:10.673924",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.645106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f43df18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.710773Z",
     "iopub.status.busy": "2024-05-23T02:10:10.710083Z",
     "iopub.status.idle": "2024-05-23T02:10:10.717435Z",
     "shell.execute_reply": "2024-05-23T02:10:10.716613Z"
    },
    "papermill": {
     "duration": 0.028385,
     "end_time": "2024-05-23T02:10:10.719859",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.691474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#A more efficient way to do this averaging\n",
    "\n",
    "#Lets create a lower triangule with ones\n",
    "a = torch.tril(torch.ones(3,3))  #tril returns the lower triangule of the 2d Array\n",
    "# a @ matrix = c  -> c_ij = sum(matrix_kj), k < = i,  (sum until element)\n",
    "\n",
    "#Divide the 1's to create an average matrix multiplication\n",
    "a = a/torch.sum(a,1, keepdim = True)\n",
    "# now, a is equivalent to the double for's from the previous chunk of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05106690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.756503Z",
     "iopub.status.busy": "2024-05-23T02:10:10.755874Z",
     "iopub.status.idle": "2024-05-23T02:10:10.770730Z",
     "shell.execute_reply": "2024-05-23T02:10:10.769570Z"
    },
    "papermill": {
     "duration": 0.035966,
     "end_time": "2024-05-23T02:10:10.773042",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.737076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2363e-08)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A more efficient way to do this averaging\n",
    "wei = torch.tril(torch.ones(T, T))   \n",
    "wei = wei/torch.sum(wei,1, keepdim = True)  #Average weights matrix\n",
    "\n",
    "xbow2 = wei @ x #(T,T) @ (B, T, C) ---> it creates a Batch dimension to wei -> (B,T,C)\n",
    "\n",
    "(xbow2-xbow).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d9f3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.810498Z",
     "iopub.status.busy": "2024-05-23T02:10:10.809334Z",
     "iopub.status.idle": "2024-05-23T02:10:10.821657Z",
     "shell.execute_reply": "2024-05-23T02:10:10.820503Z"
    },
    "papermill": {
     "duration": 0.033635,
     "end_time": "2024-05-23T02:10:10.824054",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.790419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2363e-08)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Version 3: Use SoftMax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #replace the coordinates of tril == 0 is true with , float('-inf')\n",
    "wei = F.softmax(wei, dim = -1) #The softmax exponentiate the value, so -inf  becomes zero, and the other values get normalized \n",
    "\n",
    "xbow3 = wei @ x\n",
    "\n",
    "# It have some interesting behaviors if we put some learning in wei\n",
    "# Its calculate how long should we go to the past.\n",
    "# the '-inf' where tril == 0 assures that we don't use information from the future\n",
    "\n",
    "(xbow3-xbow).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdf059",
   "metadata": {
    "papermill": {
     "duration": 0.022465,
     "end_time": "2024-05-23T02:10:10.863899",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.841434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Self attention\n",
    "- We want to train the wei matrix!\n",
    "- Each token/node emits 2 vectors: One \"query\" (what I'm looking for) and one \"key\" (what do I contain).\n",
    "- The way do affinities (the mix of information) -> we do a dot product of keys and queries.  My query (dot product) with all the keys of all the other tokens becomes \"wei\"\n",
    "- If query.key are align, they will interact with a very high amount, so I get to know more about these tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6275d90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:10.911993Z",
     "iopub.status.busy": "2024-05-23T02:10:10.911084Z",
     "iopub.status.idle": "2024-05-23T02:10:10.942218Z",
     "shell.execute_reply": "2024-05-23T02:10:10.941202Z"
    },
    "papermill": {
     "duration": 0.058056,
     "end_time": "2024-05-23T02:10:10.944824",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.886768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Version 4: Self-attention\n",
    "B,T,C = 4, 8, 32  #We encode each character into a 32D space\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False) #Channel is the input, the output is head_size shapped: C @ W_key = keys\n",
    "query = nn.Linear(C, head_size, bias = False) #C @ W_query = query \n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "#Each vector has the same shape! (the same axies?). That's how we ensure that the dot product can be applied\n",
    "\n",
    "k = key(x) #(B, T, 16)\n",
    "q = query(x) #(B, T, 16)  #No information is crossed in these lines\n",
    "\n",
    "wei = q @ k.transpose(-2,-1)  # we need to transpose the last two dimensions 0 -> (B,T,16) @ (B,16,T) ->> (B,T,T)\n",
    "wei*head_size**(-0.5)  # We do this to initiallize better. Otherwise, we don't control the variance. \n",
    "#Also, softmax is more less sensible if all the values are scalated, and the output tends to go with the highest one\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "v = value(x) #We don't aggregate the wei direct to x, we first pass x into v (values), and we aggregate the values\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92246afc",
   "metadata": {
    "papermill": {
     "duration": 0.0177,
     "end_time": "2024-05-23T02:10:10.982167",
     "exception": false,
     "start_time": "2024-05-23T02:10:10.964467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. (In a 8 block size: token 2 talks with token 1 and 2. Token 3 sees token 1,2 and 3).\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540db6c",
   "metadata": {
    "papermill": {
     "duration": 0.018297,
     "end_time": "2024-05-23T02:10:11.019723",
     "exception": false,
     "start_time": "2024-05-23T02:10:11.001426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why Multi-Head Self-Attention is More Efficient\n",
    "\n",
    "### Concept of Multi-Head Self-Attention:\n",
    "\n",
    "   - Multi-head self-attention runs multiple attention mechanisms in parallel, each with different projections of query (Q), key (K), and value (V) vectors.\n",
    "   - The outputs of these attention heads are concatenated and projected to form the final output.\n",
    "\n",
    "### Advantages of Multi-Head Self-Attention:\n",
    "\n",
    "    - Capturing Multiple Representations:\n",
    "        - Diversity of Attention: Each head can focus on different parts of the input sequence, capturing various aspects such as grammatical and semantic relationships.\n",
    "        - Reduced Redundancy: Multiple heads capture complementary information, reducing redundancy in the representation of relationships.\n",
    "\n",
    "    - Improved Learning of Complex Patterns:\n",
    "        - Hierarchical Patterns: Different heads can learn different hierarchical and granular patterns within the data.\n",
    "        - Focus on Different Positions: Each head can attend to different parts of the input, combining multiple contexts simultaneously.\n",
    "\n",
    "    - Reduced Dimension of Each Head:\n",
    "        - Lower Dimension, Lower Cost: Operating on smaller dimensions can be computationally less expensive than a single larger dimension.\n",
    "        - Efficient Parallelization: Multi-head attention allows more efficient parallel operations, better leveraging modern computational resources.\n",
    "        \n",
    "Lets rewrite the code to use multi-head s.a\n",
    "\n",
    "(initial loss, without SA: 2.5727508068084717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247455ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:11.057471Z",
     "iopub.status.busy": "2024-05-23T02:10:11.057085Z",
     "iopub.status.idle": "2024-05-23T02:10:11.067856Z",
     "shell.execute_reply": "2024-05-23T02:10:11.066586Z"
    },
    "papermill": {
     "duration": 0.032624,
     "end_time": "2024-05-23T02:10:11.070320",
     "exception": false,
     "start_time": "2024-05-23T02:10:11.037696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        #wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f994ee40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:11.111062Z",
     "iopub.status.busy": "2024-05-23T02:10:11.110338Z",
     "iopub.status.idle": "2024-05-23T02:10:11.119777Z",
     "shell.execute_reply": "2024-05-23T02:10:11.118820Z"
    },
    "papermill": {
     "duration": 0.034139,
     "end_time": "2024-05-23T02:10:11.122409",
     "exception": false,
     "start_time": "2024-05-23T02:10:11.088270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  #Creates a module - list of layers - with head\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)   #Concatenate all the heads outputs\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f89e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:11.166580Z",
     "iopub.status.busy": "2024-05-23T02:10:11.165752Z",
     "iopub.status.idle": "2024-05-23T02:10:11.181295Z",
     "shell.execute_reply": "2024-05-23T02:10:11.180173Z"
    },
    "papermill": {
     "duration": 0.038304,
     "end_time": "2024-05-23T02:10:11.184150",
     "exception": false,
     "start_time": "2024-05-23T02:10:11.145846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets update the Bigram\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #Both position_emb.. and token_emb live in the same vector - \"world\"\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.ffw = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets = None):  # THIS CALLS __call__ method. So, m(xb, yb) = m.__call__(xb,yb)\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = 'cpu')) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)  #Lets mix the information of the character with the position\n",
    "        x = self.sa_heads(x) # (B,T,C)   -> In this layer, we didn't cross information between the tokens. The input only depends on x\n",
    "        x = self.ffw(x) # So, we use this feedforward layer to cross the information!!!  \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:      \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #crop idc to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B,C) ---> the -1 is to use only the last character - its a bigram\n",
    "            #apply softmas to get probabilites\n",
    "            probs = F.softmax(logits, dim = -1) #(B,C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples =1) #(B,1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b3ee94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:10:11.222314Z",
     "iopub.status.busy": "2024-05-23T02:10:11.221896Z",
     "iopub.status.idle": "2024-05-23T02:11:09.903200Z",
     "shell.execute_reply": "2024-05-23T02:11:09.901973Z"
    },
    "papermill": {
     "duration": 58.703231,
     "end_time": "2024-05-23T02:11:09.905730",
     "exception": false,
     "start_time": "2024-05-23T02:10:11.202499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1552, val loss 4.1576\n",
      "step 250: train loss 2.8820, val loss 2.9006\n",
      "step 500: train loss 2.6657, val loss 2.6614\n",
      "step 750: train loss 2.5754, val loss 2.5611\n",
      "step 1000: train loss 2.5226, val loss 2.5053\n",
      "step 1250: train loss 2.4673, val loss 2.4684\n",
      "step 1500: train loss 2.4377, val loss 2.4384\n",
      "step 1750: train loss 2.4157, val loss 2.4169\n",
      "step 2000: train loss 2.3932, val loss 2.3954\n",
      "step 2250: train loss 2.3621, val loss 2.3718\n",
      "step 2500: train loss 2.3593, val loss 2.3751\n",
      "step 2750: train loss 2.3367, val loss 2.3456\n",
      "step 3000: train loss 2.3351, val loss 2.3362\n",
      "step 3250: train loss 2.3209, val loss 2.3197\n",
      "step 3500: train loss 2.2993, val loss 2.3183\n",
      "step 3750: train loss 2.2867, val loss 2.3198\n",
      "step 4000: train loss 2.2846, val loss 2.3025\n",
      "step 4250: train loss 2.2679, val loss 2.2913\n",
      "step 4500: train loss 2.2771, val loss 2.2917\n",
      "step 4750: train loss 2.2727, val loss 2.2959\n",
      "step 4999: train loss 2.2643, val loss 2.2787\n"
     ]
    }
   ],
   "source": [
    "#Lets optmize\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "n_embd = 32\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "    \n",
    "# create a PyTorch optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "max_iters = 5000\n",
    "eval_interval = 250 #Frequency of evalutation\n",
    "eval_iters = 200\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "564cb2d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:09.947965Z",
     "iopub.status.busy": "2024-05-23T02:11:09.946927Z",
     "iopub.status.idle": "2024-05-23T02:11:10.352254Z",
     "shell.execute_reply": "2024-05-23T02:11:10.350901Z"
    },
    "papermill": {
     "duration": 0.42938,
     "end_time": "2024-05-23T02:11:10.355095",
     "exception": false,
     "start_time": "2024-05-23T02:11:09.925715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEY:\n",
      "Why hopleteen lard do-ude fret Corfell, prrou hon of is my wonaise ing!\n",
      "\n",
      "prtre robusithere wen thame pre.\n",
      "\n",
      "IUUSRIZY ICI:\n",
      "Of sarden:\n",
      "Baye blaves afpe, his mas blosht, one!\n",
      "SUSORCESTUCEORUS:\n",
      "Tou giodd in\n",
      "Andsy amy uly ser les.\n",
      "\n",
      "II alllll vang the bary thiong ming?\n",
      "\n",
      "QEY:\n",
      "UCLAB:\n",
      "The hirkaye.\n",
      "\n",
      "Wamis\n"
     ]
    }
   ],
   "source": [
    "# Loss = 2.24 way lower than before\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1775278",
   "metadata": {
    "papermill": {
     "duration": 0.019442,
     "end_time": "2024-05-23T02:11:10.394624",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.375182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extras to the model\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:856/1*ZCFSvkKtppgew3cc7BIaug.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07744599",
   "metadata": {
    "papermill": {
     "duration": 0.019572,
     "end_time": "2024-05-23T02:11:10.433893",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.414321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lets take a look at the transform diagram. \n",
    "\n",
    "## 1) Blocks\n",
    "We may notice that there is a block that repeats itself:\n",
    "- Multi-head Attention + Add & Norm + Multi-head Attention + Add & Norm + Feed Forward + Add & Norm\n",
    "\n",
    "The block is basically -> Communication + Computation\n",
    "Communication = MultiHead Attention\n",
    "Computation = Feed Forward\n",
    "\n",
    "The Multi-head attention and feed forward were already implemented. But we need to go further!\n",
    "\n",
    "## 2) Residual Connections\n",
    "Before each multi-head attention, the input splits and it is added at the end. This is the RESIDUAL CONNECTIONS method, in which we add the transformed data to the input:\n",
    "\n",
    "> x  -> x + residual(x)\n",
    "\n",
    "> with: residual = feedforward(attention (x))\n",
    "\n",
    "this is usefull because when we backpropagate, the supervision/gradient from the loss hop (jumps) through every addition node, directly to the input. This helps extremelly the optimization process. Usually, the residual blocks are initialize in a way that they contribute very little to the gradient.\n",
    "\n",
    "For this, we need to add a linear layer after the multihead and feed forward layers to add to the input. The output of these layers should be the same dimension of the input!\n",
    "\n",
    "## 3) LayerNorm (Add & Norm layer)\n",
    "It's very simmilar to batch normalization. But instead of normalizing with respect to the batch dimension, we normalize with respect of the time (tokens). But, we don't need to keep track of batch mean and variance.\n",
    "\n",
    "Nowadays its more commom to add the layernorm before the SA layers.\n",
    "\n",
    "## 4) Dropout Layers\n",
    "To scale the model, we should use techniques to avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc732db",
   "metadata": {
    "papermill": {
     "duration": 0.019281,
     "end_time": "2024-05-23T02:11:10.472893",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.453612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e53206ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:10.515115Z",
     "iopub.status.busy": "2024-05-23T02:11:10.514409Z",
     "iopub.status.idle": "2024-05-23T02:11:10.520728Z",
     "shell.execute_reply": "2024-05-23T02:11:10.519858Z"
    },
    "papermill": {
     "duration": 0.030621,
     "end_time": "2024-05-23T02:11:10.523172",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.492551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SET hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 250\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 192\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71072e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:10.565641Z",
     "iopub.status.busy": "2024-05-23T02:11:10.564983Z",
     "iopub.status.idle": "2024-05-23T02:11:10.874740Z",
     "shell.execute_reply": "2024-05-23T02:11:10.873872Z"
    },
    "papermill": {
     "duration": 0.33453,
     "end_time": "2024-05-23T02:11:10.877622",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.543092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PREPARE THE DATA\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab5364e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:10.919927Z",
     "iopub.status.busy": "2024-05-23T02:11:10.919304Z",
     "iopub.status.idle": "2024-05-23T02:11:10.926581Z",
     "shell.execute_reply": "2024-05-23T02:11:10.925569Z"
    },
    "papermill": {
     "duration": 0.031275,
     "end_time": "2024-05-23T02:11:10.929024",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.897749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using the average loss of epochs\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9a578",
   "metadata": {
    "papermill": {
     "duration": 0.020014,
     "end_time": "2024-05-23T02:11:10.968991",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.948977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining the Classes that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142f632e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.011026Z",
     "iopub.status.busy": "2024-05-23T02:11:11.010616Z",
     "iopub.status.idle": "2024-05-23T02:11:11.021126Z",
     "shell.execute_reply": "2024-05-23T02:11:11.019731Z"
    },
    "papermill": {
     "duration": 0.034605,
     "end_time": "2024-05-23T02:11:11.023470",
     "exception": false,
     "start_time": "2024-05-23T02:11:10.988865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Single Head Class:\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f5b2e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.065166Z",
     "iopub.status.busy": "2024-05-23T02:11:11.063995Z",
     "iopub.status.idle": "2024-05-23T02:11:11.072760Z",
     "shell.execute_reply": "2024-05-23T02:11:11.071586Z"
    },
    "papermill": {
     "duration": 0.032334,
     "end_time": "2024-05-23T02:11:11.075396",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.043062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MultiHead Attention block --> Calls the single head\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) \n",
    "        self.proj = nn.Linear(n_embd, n_embd) #FOR THE RESIDUAL CONNECTION \n",
    "        self.dropout = nn.Dropout(dropout) # For regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e78b1c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.117418Z",
     "iopub.status.busy": "2024-05-23T02:11:11.116682Z",
     "iopub.status.idle": "2024-05-23T02:11:11.123306Z",
     "shell.execute_reply": "2024-05-23T02:11:11.122457Z"
    },
    "papermill": {
     "duration": 0.030371,
     "end_time": "2024-05-23T02:11:11.125741",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.095370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FeedFoward -> The compute part of the transform\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  #In the paper of the residual connection, they expanded this lawer by a factor of 4\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  #Now we apply the residual connection, but shrinking the dimension.\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebacd198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.167539Z",
     "iopub.status.busy": "2024-05-23T02:11:11.166775Z",
     "iopub.status.idle": "2024-05-23T02:11:11.174613Z",
     "shell.execute_reply": "2024-05-23T02:11:11.173719Z"
    },
    "papermill": {
     "duration": 0.031789,
     "end_time": "2024-05-23T02:11:11.177266",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.145477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #communication -> We add the Layer norm BEFORE self attention.\n",
    "        x = x + self.ffwd(self.ln2(x)) #computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73070df3",
   "metadata": {
    "papermill": {
     "duration": 0.019406,
     "end_time": "2024-05-23T02:11:11.216456",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.197050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc6b91ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.258376Z",
     "iopub.status.busy": "2024-05-23T02:11:11.257662Z",
     "iopub.status.idle": "2024-05-23T02:11:11.271306Z",
     "shell.execute_reply": "2024-05-23T02:11:11.270142Z"
    },
    "papermill": {
     "duration": 0.037775,
     "end_time": "2024-05-23T02:11:11.274172",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.236397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class chatGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81b6b7",
   "metadata": {
    "papermill": {
     "duration": 0.019405,
     "end_time": "2024-05-23T02:11:11.313477",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.294072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Time to run! Optimization + Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1df95801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:11:11.354771Z",
     "iopub.status.busy": "2024-05-23T02:11:11.354271Z",
     "iopub.status.idle": "2024-05-23T02:49:04.697445Z",
     "shell.execute_reply": "2024-05-23T02:49:04.694819Z"
    },
    "papermill": {
     "duration": 2273.388267,
     "end_time": "2024-05-23T02:49:04.721429",
     "exception": false,
     "start_time": "2024-05-23T02:11:11.333162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.703425 M parameters\n",
      "step 0: train loss 4.3672, val loss 4.3709\n",
      "step 250: train loss 2.4204, val loss 2.4262\n",
      "step 500: train loss 2.2235, val loss 2.2456\n",
      "step 750: train loss 2.0765, val loss 2.1133\n",
      "step 1000: train loss 1.9399, val loss 2.0150\n",
      "step 1250: train loss 1.8575, val loss 1.9574\n",
      "step 1500: train loss 1.7876, val loss 1.9005\n",
      "step 1750: train loss 1.7283, val loss 1.8634\n",
      "step 2000: train loss 1.6863, val loss 1.8368\n",
      "step 2250: train loss 1.6466, val loss 1.8057\n",
      "step 2500: train loss 1.6266, val loss 1.7913\n",
      "step 2750: train loss 1.5965, val loss 1.7614\n",
      "step 2999: train loss 1.5687, val loss 1.7330\n",
      "\n",
      "God fair-night it: if thou sauful wit;\n",
      "But do the heart-fath, and lave henry as as this:\n",
      "In royal of hird be mear.\n",
      "\n",
      "CAMPSILLO:\n",
      "As shows do in to be, to very and Romean us;\n",
      "Is no hands are at py Herefold. \n",
      "AGRELIZ:\n",
      "Good, fond is ind years lifed to Wury her wirpit you?\n",
      "\n",
      "MARCINE:\n",
      "So muaatch wakes gondes breats staid!\n",
      "\n",
      "LYUCENTIOLBE:\n",
      "Jult quick apter the offiers real of mighturn'd:\n",
      "Harkth whulS he's not\n",
      "You now to shough have buttler: whell was daucter, for and\n",
      "vastnents did champ-shraws.\n",
      "When like of them graves! O, lamble: them true son,\n",
      "Pucked, sirous, they undeor to-deeds! whose.\n",
      "\n",
      "LANRY:\n",
      "Humst ormand me.\n",
      "\n",
      "RASHAR\n",
      "\n",
      "PARLILIF:God met in done?\n",
      "\n",
      "MENENIUS:\n",
      "Must'lor! Weral,, What ,e I dame by ve, some:\n",
      "My fear him.\n",
      "\n",
      "LARDWICK:\n",
      "So, give God, but, woe'll see ddUneptates be is\n",
      "Shadow so trown doth of subjest of heers lady.\n",
      "\n",
      "RICHARMIO:\n",
      "My us! Nay I cursue deam, that's have wimen will Julan,\n",
      "York hath do oot frul risch it a controte\n",
      "Blove drethine souch ahous all not, if\n",
      "Who hack, wy friend, when wrestle I much thoubt\n",
      "'Tis gracediam, the new, man\n",
      "And musted I therrowll king exceredy were so her\n",
      "them, you parttentle the toemowert\n",
      "Thirks, true in down rivil, the give more hese wince ands\n",
      "Elcond might either oucher an errumed of batter'd\n",
      "and an devired. Your ford fearstul what that, natune\n",
      "Wella to such\n",
      "To my faster prison my duy strainly tormant dos.\n",
      "\n",
      "CAPULESTER:\n",
      "KING EDwIX:\n",
      "How his veneme, in then be the kin\n",
      "hath spatiser'd, If royful sorreemi told quast at tile\n",
      "Our handst the devery king on thy long his\n",
      "stour nevenstice then, hard, life thy no wiltine.\n",
      "\n",
      "My king:\n",
      "Gover ward, and is men!\n",
      "\n",
      "YORK:\n",
      "\n",
      "How she lead\n",
      "Dove me; for a may--Uskickiencey's to ranned.\n",
      "\n",
      "QUEEN ELIZABETHARD:\n",
      "Ronoouns west ears for resodency, to there holse;\n",
      "Lo, they nowwite whose rord gant backmess less.\n",
      "\n",
      "NORTUS:\n",
      "Hirm shalt me sir he't.\n",
      "\n",
      "Firster:\n",
      "How shalle shast ten shon him, it thou sen,\n",
      "May Henry him Lance's sumptich:\n",
      "Tutch the wordss may so calron,---\n",
      "\n",
      "Second oven upose pardusmer anote.\n",
      "\n",
      "ESCALANUS:\n",
      "Sillince I wor\n"
     ]
    }
   ],
   "source": [
    "model = chatGPT()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b877c8",
   "metadata": {
    "papermill": {
     "duration": 0.020607,
     "end_time": "2024-05-23T02:49:04.762830",
     "exception": false,
     "start_time": "2024-05-23T02:49:04.742223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5012012,
     "sourceId": 8419265,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2369.886796,
   "end_time": "2024-05-23T02:49:06.114363",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T02:09:36.227567",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
